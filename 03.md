# 3 Evaluation Methodology
Catastrophic failures:
	- encourage a man to commit suicide
	- offer lawyers false evidence
	- give a passenger false information

Figuring out evaluation can take up the majority of effort.

This chapter: evaluation methods(how they work, their limitations)
Next chapter: use them to select models and build a evaluation pipeline.

To mitigate risks, need to enhance visibility into its failures.

Instead of word of mouth or eyeballing the results, systematic evaluation makes the results more reliable.

The metrics to evaluate language models, entropy and perplexity.

Cover best practices for how to tackle open-ended FM.

Using AI to evaluate AI responses is rising.

## Challenges of Evaluating FM
Why is evaluating foundation models more challenging than ML models?

1. it's harder to evaluate a more intelligent AI models.
2. Foundation models are open-ended, it's impossible to curate a comprehensive list of correct outputs.
3. Most FM are black boxes, because of the unknown details of FM(the model architecture, training
   data, and the training process) or the lack of expertise of developers.
4. As AI progresses, benchmarks need to evolve to catch up.

Evaluation takes on the added responsibility of exploring the potential and limitations of AI.

The good news:
	- more new methods and benchmarks
	- more evaluation repositories on Github
The bad news:
	- It lags behinds the rest of AI engineering pipeline, like algorithms.
	- people eyeballed the results to evaluate their AI applications

## Understanding Language Modeling Metrics
The language model is still the component of FM, so understanding metrics can be quite useful.

The metrics popularized by Shannon: cross entropy(bit-per-character,bits-per-byte), perplexity.

Cross entropy, perplexity, BPC, and BPB(given one and necessary info, can compute the other),
	they can be used to generate sequences of tokens, including non-text.

The more statistical information that a model can capture, the better it is at predicting.
eg. I like drinking (tea or charcoal?)

The closer your data is to training data, the better it performs.

Focus on how to interpret these metrics, may skip the math part.

### Entropy
Entropy: how much information a token carries on average?

eg: 
- up(up or down), entropy is 1, 1 bits to represent
- up left(up or down, left or right), entropy is 2, 2 bits to represent

It's easier to predict the language carrying less entropy.

### Cross Entropy
Cross entropy: how difficult is it to predict what comes next.
1. training data's predictability(training data's entropy)
2. the distribution captured by LM diverges from the distribution of training data

Cross entropy: H(P, Q) = H(P) + D(P| |Q)
The KL divergence of Q with P can be 0, if the LM learns perfectly from its training data.

### Bits-per-Character and Bits-per-Byte

### Perplexity
Perplexity is the exponential of entropy and cross entropy.
PPL(P) = 2^H(P)
PPL(P,Q) = 2^H(P,Q)

PPL(P, Q) = e^H(P,Q)

### Perplexity Interpretation and Use Cases
Cross entropy, perplexity, BPC, and BPB are variations of LM's predictive accuracy measurements.

General rules about perplexity:
- More structured data gives lower expected perplexity
- The bigger the vocabulary, the higher the perplexity
- The longer the context length, the lower the perplexity

Perplexity can be used to detect whether a text was in a model's training data.(If it's included, the perplexity can be low).
deduplication of training data: 
	eg.Add new data to the existing training dataset only if the perplexity of the new data is high.

Perplexity is the highest for unpredictable texts.

Perplexity is a proxy for understanding the model's performance on downstream tasks.

## Exact Evaluation

## AI as a Judge

## Ranking Models with Comparative Evaluation

## Summary
Stronger AI models have higher potential for catastrophic failures,
	so this chapter focused on approaches to automatic evalutions.

Investments in evaluation still lag behind in model and application development.

Interpret metrics(perplexity and cross entropy) and leverage thme in evaluation and data processing.

Evaluation approaches:
	- exact: functional correctness, similarity scores
	- subjective: AI as a judge

AI judges should be supplemented with exact evaluation, extra and human evaluation.

Comparative evaluation and the post-training alignment process need expensive preference signals,
which motivated specialized AI judges(predict which response users prefer).

Many teams tried to incorporate AI as a judge and comparative evaluation into evaluation pipelines.

:wusd
- approaches to automatic evaluation
	- functional correctness(exact)
	- similarity scores(exact)
	- AI as a judge(subjective)
	- human evaluation
	- comparative evaluation
- metrics: perplexity and cross entropy
- preference: specialized AI judges that predict which response users prefer
- evaluation pipelines
