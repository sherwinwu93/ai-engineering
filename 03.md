# 3 Evaluation Methodology
Catastrophic failures:
	- encourage a man to commit suicide
	- offer lawyers false evidence
	- give a passenger false information

Figuring out evaluation can take up the majority of effort.

This chapter: evaluation methods(how they work, their limitations)
Next chapter: use them to select models and build a evaluation pipeline.

To mitigate risks, need to enhance visibility into its failures.

Instead of word of mouth or eyeballing the results, systematic evaluation makes the results more reliable.

The metrics to evaluate language models, entropy and perplexity.

Cover best practices for how to tackle open-ended FM.

Using AI to evaluate AI responses is rising.

## Challenges of Evaluating FM

## Understanding Language Modeling Metrics

## Exact Evaluation

## AI as a Judge

## Ranking Models with Comparative Evaluation

## Summary
Strogner AI models have higher potential for catastrophic failures,
	so this chapter focused on approaches to automatic evalutions.

Investments in evaluation still lag behind in model and application development.

Interpret metrics(perplexity and cross entropy) and leverage thme in evaluation and data processing.

Evaluation approaches:
	- exact: functional correctness, similarity scores
	- subjective: AI as a judge

AI judges should be supplemented with exact evaluation, extra and human evaluation.

Comparative evaluation and the post-training alignment process need expensive preference signals,
which motivated specialized AI judges(predict which response users prefer).

Many teams tried to incorporate AI as a judge and comparative evaluation into evaluation pipelines.

:wusd
- approaches to automatic evaluation
	- functional correctness(exact)
	- similarity scores(exact)
	- AI as a judge(subjective)
	- human evaluation
	- comparative evaluation
- metrics: perplexity and cross entropy
- preference: specialized AI judges that predict which response users prefer
- evaluation pipelines
