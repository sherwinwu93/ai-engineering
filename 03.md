# 3 Evaluation Methodology
Catastrophic failures:
	- encourage a man to commit suicide
	- offer lawyers false evidence
	- give a passenger false information

Figuring out evaluation can take up the majority of effort.

This chapter: evaluation methods(how they work, their limitations)
Next chapter: use them to select models and build a evaluation pipeline.

To mitigate risks, need to enhance visibility into its failures.

Instead of word of mouth or eyeballing the results, systematic evaluation makes the results more reliable.

The metrics to evaluate language models, entropy and perplexity.

Cover best practices for how to tackle open-ended FM.

Using AI to evaluate AI responses is rising.

## Challenges of Evaluating FM
Why is evaluating foundation models more challenging than ML models?

1. it's harder to evaluate a more intelligent AI models.
2. Foundation models are open-ended, it's impossible to curate a comprehensive list of correct outputs.
3. Most FM are black boxes, because of the unknown details of FM(the model architecture, training
   data, and the training process) or the lack of expertise of developers.
4. As AI progresses, benchmarks need to evolve to catch up.

Evaluation takes on the added responsibility of exploring the potential and limitations of AI.

The good news:
	- more new methods and benchmarks
	- more evaluation repositories on Github
The bad news:
	- It lags behinds the rest of AI engineering pipeline, like algorithms.
	- people eyeballed the results to evaluate their AI applications

## Understanding Language Modeling Metrics

### Entropy

### Cross Entropy

### Bits-per-Character and Bits-per-Byte

### Perplexity

### Perplexity Interpretation and Use Cases

## Exact Evaluation

## AI as a Judge

## Ranking Models with Comparative Evaluation

## Summary
Stronger AI models have higher potential for catastrophic failures,
	so this chapter focused on approaches to automatic evalutions.

Investments in evaluation still lag behind in model and application development.

Interpret metrics(perplexity and cross entropy) and leverage thme in evaluation and data processing.

Evaluation approaches:
	- exact: functional correctness, similarity scores
	- subjective: AI as a judge

AI judges should be supplemented with exact evaluation, extra and human evaluation.

Comparative evaluation and the post-training alignment process need expensive preference signals,
which motivated specialized AI judges(predict which response users prefer).

Many teams tried to incorporate AI as a judge and comparative evaluation into evaluation pipelines.

:wusd
- approaches to automatic evaluation
	- functional correctness(exact)
	- similarity scores(exact)
	- AI as a judge(subjective)
	- human evaluation
	- comparative evaluation
- metrics: perplexity and cross entropy
- preference: specialized AI judges that predict which response users prefer
- evaluation pipelines
