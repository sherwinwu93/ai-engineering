# 2 Understanding Foundation Models
What model to use and how to adapt it.

Design decisions with consequential impact on downstream applications.

Differences in foundation model can be traced back to decisions about training data, 
  model architecture and size, post-trained to align with human preferences.

data

The transformer architecture dominates.

What's human preference, how it can be learned by a model?

Sampling is how a model chooses an output from all options, which is often overlooked.

Revisit this chapter if you're confused with those concepts.

## Training Data

### Multilingual Models

### Domain-Specific Models

## Modeling

### Model Architecture

### Model Size

## Post-Training

### Supervised Finetuning

### Preference Finetuning

## Sampling

### Sampling Fundamentals

### Sampling Strategies

### Test Time Compute

### Structured Outputs

### The Probabilistic Nature of AI

# Summary
Core design decisions when building a foundation model, help you determine what modes and how to use them.

Training data affects a model's performance, especially low-resource languages, specific domains.

Model architecture(transformer, its problems and its limitations) and model size.

The scale of a model: parameters, training tokens, FLOPs.

If the low quality of training data and self-supervision during pre-training make it doesn't align with what users want,
it can be addressed by post-training: supervised finetuning and preference finetuning.

Sampling makes it great for creative tasks and fun to talk to, however, it also causes inconsistency and hallucinations.

Systematic AI engineering:
1. Establish a solid evaluation pipeline to help detect failures and unexpected changes.

:wusd
- Core design decisions
- Training data
- Model architecture and model size
- The scale of a model
- Self-supervision, pre-training, post-training
  supervised finetuning and preference finetuning
- sampling, creative, inconsistency
- Systematic AI engineering