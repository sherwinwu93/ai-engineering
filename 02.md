# 2 Understanding Foundation Models
What model to use and how to adapt it.

Design decisions with consequential impact on downstream applications.

Differences in foundation model can be traced back to decisions about training data, 
  model architecture and size, post-trained to align with human preferences.

data

The transformer architecture dominates.

What's human preference, how it can be learned by a model?

Sampling is how a model chooses an output from all options, which is often overlooked.

Revisit this chapter if you're confused with those concepts.

## Training Data
AI model only performs well on data it was trained on.

Model developers often doesn't have data that meets their needs.

Common Crawl offers data crawled from web pages, Google provides a clean subset of it.

--

--

--

This section focuses on curating data for specific languages and domains.

Finetune foundation models on top of general-purpose models.

A smaller amount of high-quality data might outperform a model trained with a  large amount of low-quality data.

### Multilingual Models
English dominates the internet.

--

General-purpose models work better for English than other language due to the dominance of English in the internet data.

Some questions in math in other language(not English) may not pass.

Not only under-representation makes it hard to learn, but also a language's structure and the culture it embodies can make it harder.

Can we just translate queries into English.
1. this requires a model that can sufficiently understand this language.
2. Translation can cause information loss.

Quality issues: Models have unexpected performance in non-English languages.

The number of tokens in the input and response is also larger in non-English languages than English.

--

--

### Domain-Specific Models
--

There haven't been many analyses in vision data.

General-purpose models are unlikely to perform drug discovery and cancer screening well because of the lack of the data.

--

--

## Modeling
Decide what architecture it follows, how may parameters it has.

### Model Architecture
This section analyzes the transformer architecture based on attention mechanism and alternatives.

#### Transformer architecture

#### Other model architecture

### Model Size
Llama-13B refer to Llama with 13 billion parameters.

--

If a model has 7 billion parameters, each of which is stored using 2 bytes,
  the GPU memory will be at least 14billion bytes(14GB).

A 90% sparse of 7B-parameter model has 700 million non-zero parameters.

An MoE model is divided into different groups(experts) of parameters, only a subset of groups is used to process each token.
eg: Mixtral 8x7B
  - only 46.7 billion parameters not 56 billion parameters due to some experts share the same parameters.
  - Its cost and speed are the same as a 12.9 billion parameter model because each layer has two experts.

--

It's also important to consider the size of the data,, like large images.

A book is better than sentences, so the number of tokens in the dataset is a better measurement.

Different models may have different tokenization processes, resulting in different number of tokens for the same dataset.
However, a token is the unit that a model operates on, so it's still the best measurement, not letters or words.

--

--

The number of token in dataset isn't the same as its number of training tokens.

The way to measure compute needed is by considering CPUs,CPUs, and TPUs.

--

--

--

50 utilization, you're doing okay. Above 70% utilization is considered great.

#### Scaling law: Building compute-optimal models

#### Scaling extrapolation

#### Scaling bottlenecks


## Post-Training

### Supervised Finetuning

### Preference Finetuning

## Sampling

### Sampling Fundamentals

### Sampling Strategies

### Test Time Compute

### Structured Outputs

### The Probabilistic Nature of AI

# Summary
Core design decisions when building a foundation model, help you determine what modes and how to use them.

Training data affects a model's performance, especially low-resource languages, specific domains.

Model architecture(transformer, its problems and its limitations) and model size.

The scale of a model: parameters, training tokens, FLOPs.

If the low quality of training data and self-supervision during pre-training make it doesn't align with what users want,
it can be addressed by post-training: supervised finetuning and preference finetuning.

Sampling makes it great for creative tasks and fun to talk to, however, it also causes inconsistency and hallucinations.

Systematic AI engineering:
1. Establish a solid evaluation pipeline to help detect failures and unexpected changes.

:wusd
- Core design decisions
- Training data
- Model architecture and model size
- The scale of a model
- Self-supervision, pre-training, post-training
  supervised finetuning and preference finetuning
- sampling, creative, inconsistency
- Systematic AI engineering